import torch
import torch.optim as optim
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
import matplotlib.pyplot as plt

# Pretrained model used to transform, scale, and normalize images that will be passed
# into our style mode. This is a model designed for large scale image recognition. 
# Not sure why the basic tut includes this model yet

# Loss calculation hyperparameter weights
alpha = 1.0
beta = 1.0

# Assign the GPU to be used as a device (speed up training)
device = torch.device("cuda" if (torch.cuda.is_available()) else "cpu")

# Load images
def load_image(img_path, transform=None, max_size=400, shape=None):
    image = Image.open(img_path).convert('RGB')
    
    if max_size:
        size = max(max(image.size), max_size)
        image = transforms.Resize(size)(image)
    
    if shape:
        image = transforms.Resize(shape)(image)
    
    if transform:
        image = transform(image).unsqueeze(0)
    
    return image

# Define image transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

# Load content and style images
content = load_image('contentImage.jpeg', transform)
style = load_image('styleImage.jpeg', transform, shape=content.shape[-2:])

# Define the model
class VGG(nn.Module):
    def __init__(self):
        super(VGG, self).__init__()
        self.req_features = ['0', '5', '10', '19', '28']
        self.model = models.vgg19(pretrained=True).features[:29]
        
    def forward(self, x):
        features = []
        for layer_num, layer in enumerate(self.model):
            x = layer(x)
            if str(layer_num) in self.req_features:
                features.append(x)
        return features

# Initialize the model
vgg = VGG().to('cuda' if torch.cuda.is_available() else 'cpu').eval()

# Function to define loss calculation
def calc_content_loss(gen_feature, orig_feature):

    # Content loss calced by adding MSE loss between content and generated feature then added
    # to the content loss
    content_l = torch.mean((gen_feature - orig_feature) ** 2)
    return content_l

# Gram Matrix to calclate the style loss by doing the mean squared difference between the 
# generated and style Gram Matrices
def calc_style_loss(gen, style):
    
    # G-matrix parameters
    _, channel, height, width = gen.shape
    
    # Get both gram matrices
    # PURPOSE - Gram Matrices capture differnt dependencies accross the matrices
    #           because it takes a V x V^T matrix and captures possible relatinoships 
    #           across matrix portions that may be lost in other methods. 
    G = torch.mm(gen.view(channel, height*width), gen.view(channel, height*width).t())
    A = torch.mm(style.view(channel, height*width), style.view(channel, height*width).t())
    
    # Calc mse loss between Gram matrices
    # G - gram matrix generated by viewing the generated output of the layer
    # A - gram matrix generated by viewing the convolutions of the provided style image 
    style_l = torch.mean((G - A) ** 2)
    return style_l

# Now using the style & content loss we can calc the overall loss
def calc_loss(gen_features, orig_features, style_features) :
    style_loss = content_loss = 0
    for gen,cont,style in zip(gen_features, orig_features, style_features):
        content_loss = calc_content_loss(gen, cont)
        style_loss = calc_style_loss(gen, style)
        
    # Where we use alpha and beta weight coefficients
    total_loss = alpha * content_loss + beta * style_loss
    return total_loss

# Define the style transfer function
def style_transfer(vgg, content, style, iterations=300, lr=0.003):
    target = content.clone().requires_grad_(True).to('cuda' if torch.cuda.is_available() else 'cpu')
    optimizer = optim.Adam([target], lr=lr)
    
    for i in range(iterations):
        target_features = vgg(target)
        content_features = vgg(content)
        style_features = vgg(style)
        
        content_loss = torch.mean((target_features[1] - content_features[1])**2)
        
        style_loss = 0
        for t, s in zip(target_features, style_features):
            _, c, h, w = t.size()
            t = t.view(c, h * w)
            s = s.view(c, h * w)
            t = torch.mm(t, t.t())
            s = torch.mm(s, s.t())
            style_loss += torch.mean((t - s)**2) / (c * h * w)
        
        loss = content_loss + style_loss * 1e6
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if i % 50 == 0:
            print(f'Iteration {i}, Loss: {loss.item()}')
    
    return target

# Perform style transfer
output = style_transfer(vgg, content, style)

# Save the output image
output_image = output.cpu().clone().squeeze(0)
output_image = transforms.ToPILImage()(output_image)
output_image.save('output_image.jpg')

# Display the output image
plt.imshow(output_image)
plt.show()